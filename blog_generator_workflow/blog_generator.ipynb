{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00c6f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (1.72.0)\n",
      "Requirement already satisfied: duckduckgo-search in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (2025.4.4)\n",
      "Collecting newspaper4k\n",
      "  Downloading newspaper4k-0.9.3.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting lxml_html_clean\n",
      "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (2.0.40)\n",
      "Requirement already satisfied: agno in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (1.2.15)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\dhanu\\appdata\\roaming\\python\\python311\\site-packages (from openai) (4.13.0)\n",
      "Requirement already satisfied: click>=8.1.8 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from duckduckgo-search) (8.1.8)\n",
      "Requirement already satisfied: primp>=0.14.0 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from duckduckgo-search) (0.14.0)\n",
      "Requirement already satisfied: lxml>=5.3.0 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from duckduckgo-search) (5.3.1)\n",
      "Requirement already satisfied: Pillow>=4.0.0 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from newspaper4k) (11.2.1)\n",
      "Requirement already satisfied: PyYAML>=5.1 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from newspaper4k) (6.0.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9.3 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from newspaper4k) (4.13.3)\n",
      "Collecting feedparser>=6.0.0 (from newspaper4k)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: nltk>=3.6.6 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from newspaper4k) (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.25 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from newspaper4k) (2.1.3)\n",
      "Requirement already satisfied: pandas>=2.1.0 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from newspaper4k) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from newspaper4k) (2.9.0.post0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from newspaper4k) (2.32.3)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from newspaper4k) (5.1.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from sqlalchemy) (3.1.1)\n",
      "Requirement already satisfied: docstring-parser in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from agno) (0.16)\n",
      "Requirement already satisfied: gitpython in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from agno) (3.1.44)\n",
      "Requirement already satisfied: pydantic-settings in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from agno) (2.8.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from agno) (1.1.0)\n",
      "Requirement already satisfied: python-multipart in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from agno) (0.0.20)\n",
      "Requirement already satisfied: rich in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from agno) (14.0.0)\n",
      "Requirement already satisfied: tomli in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from agno) (2.2.1)\n",
      "Requirement already satisfied: typer in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from agno) (0.15.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from beautifulsoup4>=4.9.3->newspaper4k) (2.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from click>=8.1.8->duckduckgo-search) (0.4.6)\n",
      "Collecting sgmllib3k (from feedparser>=6.0.0->newspaper4k)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: certifi in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from nltk>=3.6.6->newspaper4k) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from nltk>=3.6.6->newspaper4k) (2024.11.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from pandas>=2.1.0->newspaper4k) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from pandas>=2.1.0->newspaper4k) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dhanu\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.6.1->newspaper4k) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from requests>=2.26.0->newspaper4k) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from requests>=2.26.0->newspaper4k) (2.3.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from tldextract>=2.0.1->newspaper4k) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from tldextract>=2.0.1->newspaper4k) (3.18.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from gitpython->agno) (4.0.12)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from rich->agno) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dhanu\\appdata\\roaming\\python\\python311\\site-packages (from rich->agno) (2.19.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from typer->agno) (1.5.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython->agno) (5.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dhanu\\miniconda3\\envs\\idk\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->agno) (0.1.2)\n",
      "Downloading newspaper4k-0.9.3.1-py3-none-any.whl (296 kB)\n",
      "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6104 sha256=512da09097bdaba9e251893d3fd5be394fa61d7efcea33e5424a85fd6635320b\n",
      "  Stored in directory: c:\\users\\dhanu\\appdata\\local\\pip\\cache\\wheels\\3b\\25\\2a\\105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, lxml_html_clean, feedparser, newspaper4k\n",
      "Successfully installed feedparser-6.0.11 lxml_html_clean-0.4.2 newspaper4k-0.9.3.1 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install openai duckduckgo-search newspaper4k lxml_html_clean sqlalchemy agno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec483211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from textwrap import dedent \n",
    "from typing import Dict,Iterator,Optional\n",
    "\n",
    "from agno.agent import  Agent \n",
    "from agno.models.azure import AzureOpenAI\n",
    "from agno.storage.sqlite import  SqliteStorage\n",
    "from agno.tools.duckduckgo import DuckDuckGoTools\n",
    "from agno.utils.log import logger\n",
    "from agno.tools.newspaper4k import Newspaper4kTools\n",
    "from agno.utils.pprint import  pprint_run_response\n",
    "from agno.workflow import RunResponse,RunEvent,workflow,Workflow\n",
    "\n",
    "from pydantic import  BaseModel,Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e676081",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsArticle(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the article.\")\n",
    "    url: str = Field(..., description=\"Link to the article.\")\n",
    "    summary: Optional[str] = Field(\n",
    "        ..., description=\"Summary of the article if available.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class SearchResults(BaseModel):\n",
    "    articles: list[NewsArticle]\n",
    "\n",
    "\n",
    "class ScrapedArticle(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the article.\")\n",
    "    url: str = Field(..., description=\"Link to the article.\")\n",
    "    summary: Optional[str] = Field(\n",
    "        ..., description=\"Summary of the article if available.\"\n",
    "    )\n",
    "    content: Optional[str] = Field(\n",
    "        ...,\n",
    "        description=\"Full article content in markdown format. None if content is unavailable.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde98d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlogPostGenerator(Workflow):\n",
    "        \"\"\"Advanced workflow for generating professional blog posts with proper research and citations.\"\"\"\n",
    "\n",
    "    description: str = dedent(\"\"\"\\\n",
    "    An intelligent blog post generator that creates engaging, well-researched content.\n",
    "    This workflow orchestrates multiple AI agents to research, analyze, and craft\n",
    "    compelling blog posts that combine journalistic rigor with engaging storytelling.\n",
    "    The system excels at creating content that is both informative and optimized for\n",
    "    digital consumption.\n",
    "    \"\"\")\n",
    "\n",
    "    # Search Agent: Handles intelligent web searching and source gathering\n",
    "    searcher: Agent = Agent(\n",
    "        model=AzureOpenAI(id=\"gpt-4o\",api_key=\"\",azure_endpoint=\"https:/\"),\n",
    "        tools=[DuckDuckGoTools()],\n",
    "        description=dedent(\"\"\"\\\n",
    "        You are BlogResearch-X, an elite research assistant specializing in discovering\n",
    "        high-quality sources for compelling blog content. Your expertise includes:\n",
    "\n",
    "        - Finding authoritative and trending sources\n",
    "        - Evaluating content credibility and relevance\n",
    "        - Identifying diverse perspectives and expert opinions\n",
    "        - Discovering unique angles and insights\n",
    "        - Ensuring comprehensive topic coverage\\\n",
    "        \"\"\"),\n",
    "        instructions=dedent(\"\"\"\\\n",
    "        1. Search Strategy ðŸ”\n",
    "           - Find 10-15 relevant sources and select the 5-7 best ones\n",
    "           - Prioritize recent, authoritative content\n",
    "           - Look for unique angles and expert insights\n",
    "        2. Source Evaluation ðŸ“Š\n",
    "           - Verify source credibility and expertise\n",
    "           - Check publication dates for timeliness\n",
    "           - Assess content depth and uniqueness\n",
    "        3. Diversity of Perspectives ðŸŒ\n",
    "           - Include different viewpoints\n",
    "           - Gather both mainstream and expert opinions\n",
    "           - Find supporting data and statistics\\\n",
    "        \"\"\"),\n",
    "        response_model=SearchResults,\n",
    "    )\n",
    "\n",
    "    # Content Scraper: Extracts and processes article content\n",
    "    article_scraper: Agent = Agent(\n",
    "        model=AzureOpenAI(id=\"gpt-4o\",api_key=\"\",azure_endpoint=\"https:/\"),\n",
    "        tools=[Newspaper4kTools()],\n",
    "        description=dedent(\"\"\"\\\n",
    "        You are ContentBot-X, a specialist in extracting and processing digital content\n",
    "        for blog creation. Your expertise includes:\n",
    "\n",
    "        - Efficient content extraction\n",
    "        - Smart formatting and structuring\n",
    "        - Key information identification\n",
    "        - Quote and statistic preservation\n",
    "        - Maintaining source attribution\\\n",
    "        \"\"\"),\n",
    "        instructions=dedent(\"\"\"\\\n",
    "        1. Content Extraction ðŸ“‘\n",
    "           - Extract content from the article\n",
    "           - Preserve important quotes and statistics\n",
    "           - Maintain proper attribution\n",
    "           - Handle paywalls gracefully\n",
    "        2. Content Processing ðŸ”„\n",
    "           - Format text in clean markdown\n",
    "           - Preserve key information\n",
    "           - Structure content logically\n",
    "        3. Quality Control âœ…\n",
    "           - Verify content relevance\n",
    "           - Ensure accurate extraction\n",
    "           - Maintain readability\\\n",
    "        \"\"\"),\n",
    "        response_model=ScrapedArticle,\n",
    "    )\n",
    "\n",
    "    # Content Writer Agent: Crafts engaging blog posts from research\n",
    "    writer: Agent = Agent(\n",
    "        model=AzureOpenAI(id=\"gpt-4o\",api_key=\"\",azure_endpoint=\"https:/\"),\n",
    "        description=dedent(\"\"\"\\\n",
    "        You are BlogMaster-X, an elite content creator combining journalistic excellence\n",
    "        with digital marketing expertise. Your strengths include:\n",
    "\n",
    "        - Crafting viral-worthy headlines\n",
    "        - Writing engaging introductions\n",
    "        - Structuring content for digital consumption\n",
    "        - Incorporating research seamlessly\n",
    "        - Optimizing for SEO while maintaining quality\n",
    "        - Creating shareable conclusions\\\n",
    "        \"\"\"),\n",
    "        instructions=dedent(\"\"\"\\\n",
    "        1. Content Strategy ðŸ“\n",
    "           - Craft attention-grabbing headlines\n",
    "           - Write compelling introductions\n",
    "           - Structure content for engagement\n",
    "           - Include relevant subheadings\n",
    "        2. Writing Excellence âœï¸\n",
    "           - Balance expertise with accessibility\n",
    "           - Use clear, engaging language\n",
    "           - Include relevant examples\n",
    "           - Incorporate statistics naturally\n",
    "        3. Source Integration ðŸ”\n",
    "           - Cite sources properly\n",
    "           - Include expert quotes\n",
    "           - Maintain factual accuracy\n",
    "        4. Digital Optimization ðŸ’»\n",
    "           - Structure for scanability\n",
    "           - Include shareable takeaways\n",
    "           - Optimize for SEO\n",
    "           - Add engaging subheadings\\\n",
    "        \"\"\"),\n",
    "        expected_output=dedent(\"\"\"\\\n",
    "        # {Viral-Worthy Headline}\n",
    "\n",
    "        ## Introduction\n",
    "        {Engaging hook and context}\n",
    "\n",
    "        ## {Compelling Section 1}\n",
    "        {Key insights and analysis}\n",
    "        {Expert quotes and statistics}\n",
    "\n",
    "        ## {Engaging Section 2}\n",
    "        {Deeper exploration}\n",
    "        {Real-world examples}\n",
    "\n",
    "        ## {Practical Section 3}\n",
    "        {Actionable insights}\n",
    "        {Expert recommendations}\n",
    "\n",
    "        ## Key Takeaways\n",
    "        - {Shareable insight 1}\n",
    "        - {Practical takeaway 2}\n",
    "        - {Notable finding 3}\n",
    "\n",
    "        ## Sources\n",
    "        {Properly attributed sources with links}\\\n",
    "        \"\"\"),\n",
    "        markdown=True,\n",
    "    )\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        topic: str,\n",
    "        use_search_cache: bool = True,\n",
    "        use_scrape_cache: bool = True,\n",
    "        use_cached_report: bool = True,\n",
    "    ) -> Iterator[RunResponse]:\n",
    "        logger.info(f\"Generating a blog post on: {topic}\")\n",
    "\n",
    "        # Use the cached blog post if use_cache is True\n",
    "        if use_cached_report:\n",
    "            cached_blog_post = self.get_cached_blog_post(topic)\n",
    "            if cached_blog_post:\n",
    "                yield RunResponse(\n",
    "                    content=cached_blog_post, event=RunEvent.workflow_completed\n",
    "                )\n",
    "                return\n",
    "\n",
    "        # Search the web for articles on the topic\n",
    "        search_results: Optional[SearchResults] = self.get_search_results(\n",
    "            topic, use_search_cache\n",
    "        )\n",
    "        # If no search_results are found for the topic, end the workflow\n",
    "        if search_results is None or len(search_results.articles) == 0:\n",
    "            yield RunResponse(\n",
    "                event=RunEvent.workflow_completed,\n",
    "                content=f\"Sorry, could not find any articles on the topic: {topic}\",\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Scrape the search results\n",
    "        scraped_articles: Dict[str, ScrapedArticle] = self.scrape_articles(\n",
    "            topic, search_results, use_scrape_cache\n",
    "        )\n",
    "\n",
    "        # Prepare the input for the writer\n",
    "        writer_input = {\n",
    "            \"topic\": topic,\n",
    "            \"articles\": [v.model_dump() for v in scraped_articles.values()],\n",
    "        }\n",
    "\n",
    "        # Run the writer and yield the response\n",
    "        yield from self.writer.run(json.dumps(writer_input, indent=4), stream=True)\n",
    "\n",
    "        # Save the blog post in the cache\n",
    "        self.add_blog_post_to_cache(topic, self.writer.run_response.content)\n",
    "\n",
    "    def get_cached_blog_post(self, topic: str) -> Optional[str]:\n",
    "        logger.info(\"Checking if cached blog post exists\")\n",
    "\n",
    "        return self.session_state.get(\"blog_posts\", {}).get(topic)\n",
    "\n",
    "    def add_blog_post_to_cache(self, topic: str, blog_post: str):\n",
    "        logger.info(f\"Saving blog post for topic: {topic}\")\n",
    "        self.session_state.setdefault(\"blog_posts\", {})\n",
    "        self.session_state[\"blog_posts\"][topic] = blog_post\n",
    "\n",
    "    def get_cached_search_results(self, topic: str) -> Optional[SearchResults]:\n",
    "        logger.info(\"Checking if cached search results exist\")\n",
    "        search_results = self.session_state.get(\"search_results\", {}).get(topic)\n",
    "        return (\n",
    "            SearchResults.model_validate(search_results)\n",
    "            if search_results and isinstance(search_results, dict)\n",
    "            else search_results\n",
    "        )\n",
    "\n",
    "    def add_search_results_to_cache(self, topic: str, search_results: SearchResults):\n",
    "        logger.info(f\"Saving search results for topic: {topic}\")\n",
    "        self.session_state.setdefault(\"search_results\", {})\n",
    "        self.session_state[\"search_results\"][topic] = search_results\n",
    "\n",
    "    def get_cached_scraped_articles(\n",
    "        self, topic: str\n",
    "    ) -> Optional[Dict[str, ScrapedArticle]]:\n",
    "        logger.info(\"Checking if cached scraped articles exist\")\n",
    "        scraped_articles = self.session_state.get(\"scraped_articles\", {}).get(topic)\n",
    "        return (\n",
    "            ScrapedArticle.model_validate(scraped_articles)\n",
    "            if scraped_articles and isinstance(scraped_articles, dict)\n",
    "            else scraped_articles\n",
    "        )\n",
    "\n",
    "    def add_scraped_articles_to_cache(\n",
    "        self, topic: str, scraped_articles: Dict[str, ScrapedArticle]\n",
    "    ):\n",
    "        logger.info(f\"Saving scraped articles for topic: {topic}\")\n",
    "        self.session_state.setdefault(\"scraped_articles\", {})\n",
    "        self.session_state[\"scraped_articles\"][topic] = scraped_articles\n",
    "\n",
    "    def get_search_results(\n",
    "        self, topic: str, use_search_cache: bool, num_attempts: int = 3\n",
    "    ) -> Optional[SearchResults]:\n",
    "        # Get cached search_results from the session state if use_search_cache is True\n",
    "        if use_search_cache:\n",
    "            try:\n",
    "                search_results_from_cache = self.get_cached_search_results(topic)\n",
    "                if search_results_from_cache is not None:\n",
    "                    search_results = SearchResults.model_validate(\n",
    "                        search_results_from_cache\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"Found {len(search_results.articles)} articles in cache.\"\n",
    "                    )\n",
    "                    return search_results\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not read search results from cache: {e}\")\n",
    "\n",
    "        # If there are no cached search_results, use the searcher to find the latest articles\n",
    "        for attempt in range(num_attempts):\n",
    "            try:\n",
    "                searcher_response: RunResponse = self.searcher.run(topic)\n",
    "                if (\n",
    "                    searcher_response is not None\n",
    "                    and searcher_response.content is not None\n",
    "                    and isinstance(searcher_response.content, SearchResults)\n",
    "                ):\n",
    "                    article_count = len(searcher_response.content.articles)\n",
    "                    logger.info(\n",
    "                        f\"Found {article_count} articles on attempt {attempt + 1}\"\n",
    "                    )\n",
    "                    # Cache the search results\n",
    "                    self.add_search_results_to_cache(topic, searcher_response.content)\n",
    "                    return searcher_response.content\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        f\"Attempt {attempt + 1}/{num_attempts} failed: Invalid response type\"\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}\")\n",
    "\n",
    "        logger.error(f\"Failed to get search results after {num_attempts} attempts\")\n",
    "        return None\n",
    "\n",
    "    def scrape_articles(\n",
    "        self, topic: str, search_results: SearchResults, use_scrape_cache: bool\n",
    "    ) -> Dict[str, ScrapedArticle]:\n",
    "        scraped_articles: Dict[str, ScrapedArticle] = {}\n",
    "\n",
    "        # Get cached scraped_articles from the session state if use_scrape_cache is True\n",
    "        if use_scrape_cache:\n",
    "            try:\n",
    "                scraped_articles_from_cache = self.get_cached_scraped_articles(topic)\n",
    "                if scraped_articles_from_cache is not None:\n",
    "                    scraped_articles = scraped_articles_from_cache\n",
    "                    logger.info(\n",
    "                        f\"Found {len(scraped_articles)} scraped articles in cache.\"\n",
    "                    )\n",
    "                    return scraped_articles\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not read scraped articles from cache: {e}\")\n",
    "\n",
    "        # Scrape the articles that are not in the cache\n",
    "        for article in search_results.articles:\n",
    "            if article.url in scraped_articles:\n",
    "                logger.info(f\"Found scraped article in cache: {article.url}\")\n",
    "                continue\n",
    "\n",
    "            article_scraper_response: RunResponse = self.article_scraper.run(\n",
    "                article.url\n",
    "            )\n",
    "            if (\n",
    "                article_scraper_response is not None\n",
    "                and article_scraper_response.content is not None\n",
    "                and isinstance(article_scraper_response.content, ScrapedArticle)\n",
    "            ):\n",
    "                scraped_articles[article_scraper_response.content.url] = (\n",
    "                    article_scraper_response.content\n",
    "                )\n",
    "                logger.info(f\"Scraped article: {article_scraper_response.content.url}\")\n",
    "\n",
    "        # Save the scraped articles in the session state\n",
    "        self.add_scraped_articles_to_cache(topic, scraped_articles)\n",
    "        return scraped_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from rich.prompt import Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c5eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompts = [\n",
    "        \"Why Cats Secretly Run the Internet\",\n",
    "        \"The Science Behind Why Pizza Tastes Better at 2 AM\",\n",
    "        \"Time Travelers' Guide to Modern Social Media\",\n",
    "        \"How Rubber Ducks Revolutionized Software Development\",\n",
    "        \"The Secret Society of Office Plants: A Survival Guide\",\n",
    "        \"Why Dogs Think We're Bad at Smelling Things\",\n",
    "        \"The Underground Economy of Coffee Shop WiFi Passwords\",\n",
    "        \"A Historical Analysis of Dad Jokes Through the Ages\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab020a69",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Get topic from user\n",
    "    topic = Prompt.ask(\n",
    "        \"[bold]Enter a blog post topic[/bold] (or press Enter for a random example)\\nâœ¨\",\n",
    "        default=random.choice(example_prompts),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the topic to a URL-safe string for use in session_id\n",
    "url_safe_topic = topic.lower().replace(\" \", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298c9333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the blog post generator workflow\n",
    "# - Creates a unique session ID based on the topic\n",
    "# - Sets up SQLite storage for caching results\n",
    "generate_blog_post = BlogPostGenerator(\n",
    "session_id=f\"generate-blog-post-on-{url_safe_topic}\",\n",
    "storage=SqliteStorage(\n",
    "    table_name=\"generate_blog_post_workflows\",\n",
    "    db_file=\"tmp/agno_workflows.db\",\n",
    "),\n",
    "debug_mode=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10f07e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the workflow with caching enabled\n",
    "# Returns an iterator of RunResponse objects containing the generated content\n",
    "blog_post: Iterator[RunResponse] = generate_blog_post.run(\n",
    "topic=topic,\n",
    "use_search_cache=True,\n",
    "use_scrape_cache=True,\n",
    "use_cached_report=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint_run_response(blog_post, markdown=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
